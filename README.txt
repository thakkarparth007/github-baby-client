The project is called Github-Baby-Client, but that was how it was supposed to work. Now it does not act 
as a client for github.

Goal of the project:
Make a database of all public repositories on Github, and find out groups among them based on their
descriptions, README files and the languages used. 

Meaning of "finding groups among the repos": To identify the major categories of the software on 
Github. The categories are NOT an input to the program. The categories are discovered by the 
program by itself - though, it won't name the categories. It will only group the software based
on the similarity between their readme's, descriptions and languages. Of course, more parameters
can be added.

There are three main modules of this system:
1. Crawler
2. Indexer
3. Clusterer

1. Crawler crawls through the publicly available repository data on Github.
2. Indexer makes the crawled data ready for the third step - clustering.
3. Clusterer does the job of grouping the repositories.

So far I have only been able to complete the crawler.
The other two parts are yet to be done.

Here's a brief idea of how the other two modules work:
Indexer :
1. Looks at the crawled data and gathers the description and languages of each repo.
2. Downloads the readme of each repo separately (this could have been done in the crawler
   but to make the crawling faster, this has been bundled with the Indexer)
3. Separates words from the description and readme, and adds the words to a database.
4. The database stores all words found, along with the number of occurances of each word
   for each repo.

Clusterer:
This uses the K-clustering technique.
To understand this technique in this context, one must understand the concept of word-vectors.
The data created by the indexer, per repo, - a list of words for along with their number
of occurences (in that repo), is a word vector.
E.g. Consider a repo with a readme+description like this:
	Meaning of "finding groups among the repos": To identify the major categories of the software on 
	Github. The categories are NOT an input to the program. The categories are discovered by the 
	program by itself - though, it won't name the categories. It will only group the software based
	on the similarity between their readme's, descriptions and languages. Of course, more parameters
	can be added.

Here, you see that the word "categories" occurs 4 times. The word "software" occurs 2 times,
while "parameters" occurs once. So, the above paragraph can be represented as a table.
One must of course ignore the details like where each word occurs in the paragraph.

Now, think of the words as "directions/dimensions", and the word-count as "coordinates".
For example, if the word vector had only 3 words, word A, word B and word C, each occuring
x, y, and z times, then the repo can be thought of as a point in 3D space, with the 
coordinates being (x,y,z).

And now that we have coordinates, we can have distances too! So, the distance between two
repositories can be calculated simply by using the Euclidean distance formula, of course,
by generalising it to N dimensions (where N is the total number of unique words found in 
either of the repositories). So, the "farther" the two repositories are, the more different
their content is. Or, similar repositories will be "closer".

Euclidean distance is just one of the many possible measures of distance. Once can look up more
about this over here: https://en.wikipedia.org/wiki/Metric_%28mathematics%29

The K-means clustering or the K-clustering technique can now be understood this way:
Generate K randomly positioned vectors (we'll call them pivote-vectors) - meaning, if 
there are totally W unique words in all of the repo data collected, then we'll be dealing 
with W-dimensional spaces. So, each vector is defined by w-coordinates. ith coordinate of 
each vector is the number of occurances of the ith word.

Then, once the K pivot vectors are generated, go through each vector in our collection
(the vectors generated by the indexer for the repos), and "connect" it with the closest
of the K points. 

Once we're done with that, we'll have K distinct groups. Each repo will be "connected" to
one pivot vector - or, each repo will be under one category. However, since the pivots
were generated randomly, this won't be accurate. So, adjust the positions of each pivot 
by placing them at the centroid of the vectors it is connected to.

Repeat the above till the positions of the pivots don't change appreciably.

---------------------------------------------------------------------------------------------

That was it about the Indexer and the Clusterer. Here're the details of the crawler.
The code for the crawler can be found in /mining/crawler/

Stuff to sniff:

1. Readme *
2. Languages *
3. Stars count
4. Subscribers count
5. Forks count
6. Contributors count *
7. Description
8. User: *
	num of repos
	location
	followers
	following
	company
	hireable
9. size
10. Issues
11. Created, Updated, Pushed Times

( * indicates an extra request to the Github API )

-------------------------

After sniffing:

Cluster the repositories based on:
	Readme, Description, Languages

Findout hot keywords from description

20M repos / (364 repos a page) * (1 for repo + 1 for languages + 1 for contributors + 1 for user)

------------------------

= ~100 hours if it's a single crawler

Let's have 5 crawlers, working simultaneously, each with its own auth credentials

1 crawler (Right Hand :P) crawls  on api.github.com/repositories and collects a list of repos
The other 4 (Left Hands :P) crawl each repo,contributors count and users (each one does all four, for each repo)

The master crawler handles the cooridination:
1. Starts the Right Hand.
	Right Hand dumps the data in the crawl database.
	Each repo is stored as a document in the 'for_clustering' collection.
		{
			id
			name (idx)
			explored ( `false`,`true`,`exploring:worker#{id}` )
			owner (string - owner name.)
		}

2. Starts the left hands.
	Each left hand gets the first unexplored document in the for_clustering collection,
	marks it as "exploring:worker#{id}" where id is the id of the left hand worker.
	It gets the {repo info,languages,contributers count,user details} of the corresponding document.
	When done fetching the data, the document is marked as explored (explored=true).

	If ther worker dies while collecting the repo data, the master marks the document it was working on,
	as unexplored. Doing this avoids fetching the same data by different workers, and also ensuring that
	no repo is left untouched.

	- The repo data is stored as a document in the 'explored' collection
		{
			id,
			name,
			description,
			owner_id,
			owner_login,
			languages: [ {name:bytes}... ],
			stargazers_count,
			subscribers_count,
			watchers,
			forks_count,
			contributers_count,
			size,
			open_issues,
			created_at,
			updated_at,
			pushed_at
		}

	- The user data is stored in the 'users' collection
		{
			id,
			login,
			public_repos (repo_count),
			public_gists,
			location,
			followers,
			following,
			company,
			hireable
		}

-------------------------

prepare_for_cluster master
	Starts X prepare_for_cluster workers.
	Each worker takes the first document from the 'for_clustering' collection,
	and reads the 'readme' and 'description', and fills the buzzwords & repo_buzzwords
	collections. Each text (readme and decription) is broken down into tokens
	by a tokenizer, and then those tokens are used in the database

	- Buzz words (obtained from readme+description) are stored in the 'buzzwords' and 'repo_buzzwords' collection
		buzzwords:
		{
			id,
			word,
			total_count
		}
		repo_buzzwords:
		{
			wordid,
			repoid,
			count 		(the number of appearances of the word in the repo)
			where		('d' or 'r' for 'description' or 'readme')
		}

Once all the repos have been readied for clustering, the Master Clusterer
must be started. 
The master clusterer makes K randomly positioned points in the 'clusters' collection:
	{
		cluster_id,
		coordinates: [ ith value is the count of the word with wordid = i ]
	}

Then the following is repeated for N times:
	Then it starts Y worker clusterers and tells them the K random points.
		
		Each worker clusterer picks the next untouched repo from the repo_buzzwords,
		and then finds the closest of the K points, and assigns to it.

	Once all workers are done, it shifts the K points to the average value of the repos assigned to it.

